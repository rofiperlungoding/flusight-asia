
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import json
from torch.utils.data import DataLoader, TensorDataset

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from flusight.processing import TimeseriesProcessor
from flusight.ml.models import FluSightTransformer

def train_transformer():
    print("üöÄ Starting Transformer Training...")

    # 1. Load Data
    # In a real scenario, this would load from Supabase or a JSON file
    # For now, we look for sequences_data.json generated by the pipeline
    data_path = os.path.join(os.path.dirname(__file__), '..', 'sequences_data.json')
    
    if not os.path.exists(data_path):
        print(f"‚ùå Data file not found: {data_path}")
        # Generating dummy data for demonstration if file missing
        print("‚ö†Ô∏è Generating dummy data for testing...")
        dates = pd.date_range(start='2020-01-01', periods=1000, freq='D')
        dummy_seqs = []
        variants = ['Clade A', 'Clade B', 'Clade C', 'Clade D', 'Clade E']
        for d in dates:
            # Simulate shifting dominance
            if d.year == 2020: v = np.random.choice(variants, p=[0.6, 0.2, 0.1, 0.05, 0.05])
            elif d.year == 2021: v = np.random.choice(variants, p=[0.2, 0.5, 0.2, 0.05, 0.05])
            else: v = np.random.choice(variants, p=[0.05, 0.1, 0.5, 0.3, 0.05])
            
            dummy_seqs.append({
                'collection_date': d,
                'mutations': [{'mutation_notation': v}], # Hacking for TimeseriesProcessor
                'variant_signature': v
            })
        sequences = dummy_seqs
    else:
        with open(data_path, 'r') as f:
            sequences = json.load(f)

    # 2. Process Data
    processor = TimeseriesProcessor(sequences)
    
    # HACK: If sequences don't have 'mutations' list with 'mutation_notation',
    # processor._define_variants might fail. Let's ensure it works or we fallback.
    # In Phase 6 pipeline, specific mutation list was not fully guaranteed in the JSON 
    # unless 'ingest-sequences' logic put it there. 
    # Phase 6 'predict.py' relied on 'sequences' table.
    
    # We will assume processor works or fix it.
    
    variants = processor._define_variants(top_k=5)
    print(f"üìä Variants: {variants}")
    
    weekly_df = processor.aggregate_weekly()
    print(f"üìÖ Time-series shape: {weekly_df.shape}")
    
    if weekly_df.empty:
        print("‚ùå No data after aggregation.")
        return

    # 3. Create Dataset
    INPUT_LEN = 52
    PRED_LEN = 12
    
    X, y = processor.create_sliding_windows(weekly_df, input_len=INPUT_LEN, pred_len=PRED_LEN)
    print(f"üì¶ Dataset shapes: X={X.shape}, y={y.shape}")
    
    if len(X) < 10:
        print("‚ùå Not enough samples to train.")
        return

    # Convert to Tensor
    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.FloatTensor(y)
    
    # Split Train/Val (Last 12 weeks for val)
    split = int(len(X) * 0.8)
    X_train, y_train = X_tensor[:split], y_tensor[:split]
    X_val, y_val = X_tensor[split:], y_tensor[split:]
    
    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)
    
    # 4. Initialize Model
    NUM_VARIANTS = len(variants) + 1 # +1 for Other
    model = FluSightTransformer(
        num_variants=NUM_VARIANTS,
        d_model=32, # Small model
        num_layers=2,
        nhead=2
    )
    
    criterion = nn.KLDivLoss(reduction='batchmean')
    optimizer = optim.AdamW(model.parameters(), lr=0.001)
    
    # 5. Training Loop
    EPOCHS = 50
    best_loss = float('inf')
    
    models_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    os.makedirs(models_dir, exist_ok=True)
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for X_batch, y_batch in train_loader:
            optimizer.zero_grad()
            output = model(X_batch)
            
            # KLDiv expects input in log-space
            log_output = torch.log(output + 1e-9)
            
            loss = criterion(log_output, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        avg_loss = total_loss / len(train_loader)
        
        # Validation
        model.eval()
        with torch.no_grad():
            val_out = model(X_val)
            val_log = torch.log(val_out + 1e-9)
            val_loss = criterion(val_log, y_val).item()
            
        if epoch % 5 == 0:
            print(f"Epoch {epoch}: Train Loss={avg_loss:.4f}, Val Loss={val_loss:.4f}")
            
        if val_loss < best_loss:
            best_loss = val_loss
            torch.save(model.state_dict(), os.path.join(models_dir, 'transformer_v1.pth'))
            
    print(f"‚úÖ Training complete. Best Val Loss: {best_loss:.4f}")
    
    # Save Metadata
    metadata = {
        'variants': variants,
        'variant_map': processor.variant_map,
        'input_len': INPUT_LEN,
        'pred_len': PRED_LEN,
        'num_variants': NUM_VARIANTS
    }
    with open(os.path.join(models_dir, 'transformer_meta.json'), 'w') as f:
        json.dump(metadata, f)

if __name__ == "__main__":
    train_transformer()
