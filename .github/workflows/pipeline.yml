name: FluSight Data Pipeline

on:
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      max_sequences:
        description: 'Maximum sequences to fetch'
        required: false
        default: '100'
      dry_run:
        description: 'Dry run (skip database writes)'
        required: false
        default: 'false'

env:
  PYTHON_VERSION: '3.11'
  SUPABASE_DB_HOST: db.zrxwptfzzsaehtpjhvij.supabase.co
  SUPABASE_DB_NAME: postgres
  SUPABASE_DB_USER: postgres
  SUPABASE_DB_PASSWORD: ${{ secrets.SUPABASE_DB_PASSWORD }}
  NCBI_EMAIL: ${{ secrets.NCBI_EMAIL }}
  NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}

jobs:
  ingest-sequences:
    name: üß¨ Ingest Sequences
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: pipeline

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: pipeline/pyproject.toml

      - name: üì¶ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install biopython requests python-dotenv pandas numpy tqdm pydantic psycopg2-binary

      - name: üî¨ Fetch and parse sequences
        id: fetch
        run: |
          python << 'EOF'
          import sys
          sys.path.insert(0, 'src')
          from flusight.ingestion.ncbi import NCBIFetcher
          import os
          import json

          email = os.environ.get('NCBI_EMAIL')
          api_key = os.environ.get('NCBI_API_KEY')
          max_sequences = int('${{ github.event.inputs.max_sequences }}' or '100')

          print(f'üìß NCBI Email: {email[:5]}***')
          print(f'üî¢ Max sequences: {max_sequences}')

          fetcher = NCBIFetcher(email=email, api_key=api_key)
          query = fetcher.build_search_query(subtype='H3N2', segment='HA', region='Asia')
          
          print(f'üîç Query: {query[:80]}...')
          
          ids = fetcher.search(query, max_results=max_sequences)
          print(f'‚úÖ Found {len(ids)} sequence IDs')

          if not ids:
              print('‚ö†Ô∏è No sequences found')
              with open('sequences_data.json', 'w') as f:
                  json.dump([], f)
              sys.exit(0)

          print(f'üì• Fetching sequences from NCBI...')
          sequences = []
          
          for i, record in enumerate(fetcher.fetch_records(ids)):
              parsed = fetcher.parse_record(record)
              sequences.append(parsed)
              if (i + 1) % 25 == 0:
                  print(f'  Processed {i + 1}/{len(ids)}...')

          print(f'‚úÖ Parsed {len(sequences)} sequences')

          with open('sequences_data.json', 'w') as f:
              json.dump(sequences, f, default=str, indent=2)

          print('üíæ Saved sequences to sequences_data.json')
          EOF

      - name: üíæ Write to Supabase (Direct SQL)
        if: github.event.inputs.dry_run != 'true'
        run: |
          python << 'EOF'
          import os
          import json
          import psycopg2
          from datetime import datetime

          # Get database connection details
          db_host = os.environ.get('SUPABASE_DB_HOST')
          db_password = os.environ.get('SUPABASE_DB_PASSWORD')

          if not db_host or not db_password:
              print('‚ö†Ô∏è Database credentials not configured - skipping')
              print('   Add SUPABASE_DB_PASSWORD to repository secrets')
              exit(0)

          # Load parsed sequences
          with open('sequences_data.json', 'r') as f:
              sequences = json.load(f)

          if not sequences:
              print('‚ö†Ô∏è No sequences to write')
              exit(0)

          print(f'üìù Connecting to database...')

          # Connect to PostgreSQL
          conn = psycopg2.connect(
              host=db_host,
              database='postgres',
              user='postgres',
              password=db_password,
              port=5432,
              sslmode='require'
          )
          conn.autocommit = True
          cur = conn.cursor()

          print(f'‚úÖ Connected! Writing {len(sequences)} sequences...')

          written = 0
          errors = 0

          for seq in sequences:
              try:
                  # Skip if missing required fields
                  if not seq.get('strain_name') or not seq.get('raw_sequence'):
                      print(f"‚è≠Ô∏è Skipping {seq.get('genbank_id')}: missing required fields")
                      errors += 1
                      continue

                  # Use INSERT ... ON CONFLICT (upsert)
                  cur.execute("""
                      INSERT INTO sequences (
                          genbank_id, strain_name, segment, subtype, 
                          raw_sequence, sequence_length, collection_date, 
                          host, source
                      ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                      ON CONFLICT (genbank_id) DO UPDATE SET
                          strain_name = EXCLUDED.strain_name,
                          raw_sequence = EXCLUDED.raw_sequence,
                          sequence_length = EXCLUDED.sequence_length,
                          collection_date = EXCLUDED.collection_date,
                          updated_at = NOW()
                  """, (
                      seq.get('genbank_id'),
                      seq.get('strain_name'),
                      seq.get('segment', 'HA'),
                      seq.get('subtype', 'H3N2'),
                      seq.get('raw_sequence'),
                      seq.get('sequence_length') or len(seq.get('raw_sequence', '')),
                      seq.get('collection_date'),
                      seq.get('host', 'Human'),
                      'ncbi'
                  ))
                  written += 1
                  
              except Exception as e:
                  print(f'‚ùå Error for {seq.get("genbank_id")}: {e}')
                  errors += 1

          print(f'‚úÖ Written: {written}')
          print(f'‚ùå Errors: {errors}')

          # Log the job
          try:
              status = 'success' if errors == 0 else 'partial'
              cur.execute("""
                  INSERT INTO pipeline_logs (job_name, status, records_processed, records_failed, message)
                  VALUES (%s, %s, %s, %s, %s)
              """, (
                  'ncbi_ingest',
                  status,
                  written,
                  errors,
                  f'Wrote {written} sequences, {errors} errors'
              ))
              print('üìã Logged to pipeline_logs')
          except Exception as e:
              print(f'‚ö†Ô∏è Could not log: {e}')

          cur.close()
          conn.close()
          print('‚úÖ Done!')
          EOF

      - name: üì§ Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sequence-data
          path: pipeline/sequences_data.json
          retention-days: 7
