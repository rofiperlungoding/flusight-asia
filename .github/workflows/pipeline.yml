name: FluSight Data Pipeline

on:
  schedule:
    - cron: '0 0 * * 0'
  workflow_dispatch:
    inputs:
      max_sequences:
        description: 'Maximum sequences to fetch'
        required: false
        default: '100'
      dry_run:
        description: 'Dry run (skip database writes)'
        required: false
        default: 'false'

env:
  PYTHON_VERSION: '3.11'
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
  NCBI_EMAIL: ${{ secrets.NCBI_EMAIL }}
  NCBI_API_KEY: ${{ secrets.NCBI_API_KEY }}

jobs:
  ingest-sequences:
    name: üß¨ Ingest Sequences
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: pipeline

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üêç Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: pipeline/pyproject.toml

      - name: üì¶ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install biopython requests python-dotenv pandas numpy tqdm pydantic httpx

      - name: üî¨ Fetch and parse sequences
        id: fetch
        run: |
          python << 'EOF'
          import sys
          sys.path.insert(0, 'src')
          from flusight.ingestion.ncbi import NCBIFetcher
          import os
          import json

          email = os.environ.get('NCBI_EMAIL')
          api_key = os.environ.get('NCBI_API_KEY')
          max_sequences = int('${{ github.event.inputs.max_sequences }}' or '100')

          print(f'üìß NCBI Email: {email[:5]}***')
          print(f'üî¢ Max sequences: {max_sequences}')

          fetcher = NCBIFetcher(email=email, api_key=api_key)
          query = fetcher.build_search_query(subtype='H3N2', segment='HA', region='Asia')
          
          print(f'üîç Query: {query[:80]}...')
          
          ids = fetcher.search(query, max_results=max_sequences)
          print(f'‚úÖ Found {len(ids)} sequence IDs')

          if not ids:
              print('‚ö†Ô∏è No sequences found')
              with open('sequences_data.json', 'w') as f:
                  json.dump([], f)
              sys.exit(0)

          print(f'üì• Fetching sequences from NCBI...')
          sequences = []
          
          for i, record in enumerate(fetcher.fetch_records(ids)):
              parsed = fetcher.parse_record(record)
              sequences.append(parsed)
              if (i + 1) % 25 == 0:
                  print(f'  Processed {i + 1}/{len(ids)}...')

          print(f'‚úÖ Parsed {len(sequences)} sequences')

          with open('sequences_data.json', 'w') as f:
              json.dump(sequences, f, default=str, indent=2)

          print('üíæ Saved sequences to sequences_data.json')
          EOF

      - name: üíæ Write to Supabase
        if: github.event.inputs.dry_run != 'true'
        run: |
          python << 'EOF'
          import os
          import json
          import httpx
          from datetime import datetime

          supabase_url = os.environ.get('SUPABASE_URL', '').rstrip('/')
          supabase_key = os.environ.get('SUPABASE_SERVICE_KEY', '')

          if not supabase_url or not supabase_key:
              print('‚ö†Ô∏è Supabase secrets not configured - skipping')
              exit(0)

          with open('sequences_data.json', 'r') as f:
              sequences = json.load(f)

          if not sequences:
              print('‚ö†Ô∏è No sequences to write')
              exit(0)

          print(f'üìù Writing {len(sequences)} sequences to Supabase...')
          print(f'   URL: {supabase_url}')

          rest_url = f'{supabase_url}/rest/v1'
          headers = {
              'apikey': supabase_key,
              'Authorization': f'Bearer {supabase_key}',
              'Content-Type': 'application/json',
              'Prefer': 'resolution=merge-duplicates,return=minimal',
          }

          written = 0
          skipped = 0
          errors = 0

          # Process in batches of 10 for efficiency
          batch_size = 10
          for i in range(0, len(sequences), batch_size):
              batch = sequences[i:i+batch_size]
              records = []
              
              for seq in batch:
                  if not seq.get('strain_name') or not seq.get('raw_sequence'):
                      skipped += 1
                      continue
                      
                  record = {
                      'genbank_id': seq.get('genbank_id'),
                      'strain_name': seq.get('strain_name'),
                      'segment': seq.get('segment', 'HA'),
                      'subtype': seq.get('subtype', 'H3N2'),
                      'raw_sequence': seq.get('raw_sequence'),
                      'sequence_length': seq.get('sequence_length') or len(seq.get('raw_sequence', '')),
                      'host': seq.get('host', 'Human'),
                      'source': 'ncbi',
                  }
                  # Add collection_date only if present
                  if seq.get('collection_date'):
                      record['collection_date'] = seq.get('collection_date')
                  
                  records.append(record)
              
              if not records:
                  continue
                  
              try:
                  response = httpx.post(
                      f'{rest_url}/sequences',
                      headers=headers,
                      params={'on_conflict': 'genbank_id'},
                      json=records,
                      timeout=60.0,
                  )
                  
                  if response.status_code in [200, 201, 204]:
                      written += len(records)
                      print(f'  ‚úÖ Batch {i//batch_size + 1}: {len(records)} records')
                  else:
                      print(f'  ‚ö†Ô∏è Batch {i//batch_size + 1} HTTP {response.status_code}: {response.text[:200]}')
                      errors += len(records)
                      
              except Exception as e:
                  print(f'  ‚ùå Batch {i//batch_size + 1} error: {e}')
                  errors += len(records)

          print(f'')
          print(f'üìä Results:')
          print(f'   ‚úÖ Written: {written}')
          print(f'   ‚è≠Ô∏è Skipped: {skipped}')
          print(f'   ‚ùå Errors: {errors}')

          # Log the job
          try:
              status = 'success' if errors == 0 else 'partial'
              log_headers = {**headers, 'Prefer': 'return=minimal'}
              httpx.post(
                  f'{rest_url}/pipeline_logs',
                  headers=log_headers,
                  json=[{
                      'job_name': 'ncbi_ingest',
                      'status': status,
                      'records_processed': written,
                      'records_failed': errors,
                      'message': f'Wrote {written}, skipped {skipped}, errors {errors}',
                  }],
                  timeout=10.0,
              )
              print('üìã Logged to pipeline_logs')
          except Exception as e:
              print(f'‚ö†Ô∏è Log error: {e}')
          
          print('‚úÖ Done!')
          EOF

      - name: üì§ Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: sequence-data
          path: pipeline/sequences_data.json
          retention-days: 7

      - name: üß¨ Detect Mutations
        if: github.event.inputs.dry_run != 'true'
        run: |
          python << 'EOF'
          import sys
          sys.path.insert(0, 'src')
          import os
          import httpx
          from flusight.processing import MutationDetector, get_mutation_summary

          supabase_url = os.environ.get('SUPABASE_URL', '').rstrip('/')
          supabase_key = os.environ.get('SUPABASE_SERVICE_KEY', '')

          if not supabase_url or not supabase_key:
              print('‚ö†Ô∏è Supabase secrets not configured - skipping')
              exit(0)

          rest_url = f'{supabase_url}/rest/v1'
          headers = {
              'apikey': supabase_key,
              'Authorization': f'Bearer {supabase_key}',
              'Content-Type': 'application/json',
          }

          # Fetch sequences that haven't been analyzed yet
          print('üìñ Fetching sequences for mutation analysis...')
          response = httpx.get(
              f'{rest_url}/sequences',
              headers=headers,
              params={
                  'select': 'id,strain_name,raw_sequence',
                  'order': 'created_at.desc',
                  'limit': '50',
              },
              timeout=30.0,
          )
          
          if response.status_code != 200:
              print(f'‚ùå Failed to fetch sequences: {response.status_code}')
              exit(1)
          
          sequences = response.json()
          print(f'üìã Found {len(sequences)} sequences to analyze')

          if not sequences:
              print('‚ÑπÔ∏è No sequences to analyze')
              exit(0)

          # Get existing mutations to track novelty
          print('üìñ Fetching existing mutations...')
          mut_response = httpx.get(
              f'{rest_url}/mutations',
              headers=headers,
              params={'select': 'mutation_notation'},
              timeout=30.0,
          )
          known_mutations = set()
          if mut_response.status_code == 200:
              known_mutations = {m['mutation_notation'] for m in mut_response.json()}
          print(f'   Known mutations: {len(known_mutations)}')

          # Initialize detector with known mutations
          detector = MutationDetector(known_mutations=known_mutations)

          # Process each sequence
          total_mutations = 0
          sequences_processed = 0
          
          for seq in sequences:
              seq_id = seq['id']
              raw_seq = seq.get('raw_sequence', '')
              
              if not raw_seq or len(raw_seq) < 100:
                  continue
              
              # Detect mutations
              mutations = detector.detect_mutations(raw_seq)
              
              if mutations:
                  # Prepare records for insertion
                  records = []
                  for m in mutations:
                      record = m.to_dict()
                      record['sequence_id'] = seq_id
                      records.append(record)
                  
                  # Upsert to mutations table
                  try:
                      upsert_headers = {**headers, 'Prefer': 'resolution=merge-duplicates,return=minimal'}
                      response = httpx.post(
                          f'{rest_url}/mutations',
                          headers=upsert_headers,
                          params={'on_conflict': 'sequence_id,position'},
                          json=records,
                          timeout=30.0,
                      )
                      
                      if response.status_code in [200, 201, 204]:
                          total_mutations += len(records)
                          sequences_processed += 1
                          print(f'  ‚úÖ {seq["strain_name"][:40]}: {get_mutation_summary(mutations)}')
                      else:
                          print(f'  ‚ö†Ô∏è {seq["strain_name"][:30]}: HTTP {response.status_code}')
                  except Exception as e:
                      print(f'  ‚ùå Error: {e}')
              else:
                  sequences_processed += 1
                  print(f'  ‚ö™ {seq["strain_name"][:40]}: No mutations')

          print(f'')
          print(f'üìä Mutation Analysis Complete:')
          print(f'   Sequences analyzed: {sequences_processed}')
          print(f'   Mutations detected: {total_mutations}')
          EOF
